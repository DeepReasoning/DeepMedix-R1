<h1 align="center">
A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning
</h1>

<p align="center">
  <a href=""><b>[ğŸ“œ Paper]</b></a> â€¢
  <a href="https://github.com/DeepReasoning/DeepMedix-R1"><b>[ğŸ± GitHub]</b></a>
  
</p>

<p align="center">
Repo for "<a href="" target="_blank">A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning</a>"
</p>

## ğŸ”¥ News

- [2025/02] ğŸ”¥ğŸ”¥ğŸ”¥ Logical reasoning evaluation study of LLMs is accepted by IEEE TKDE!

## ğŸ“– Introduction

<p align="center">
    <img src="evaluation.png" alt="scaling" width="400">
</p>

In this paper, in-depth evaluations are conducted on logical reasoning tasks, discussing whether LLMs are really good logical reasons.
> - First, the logical reasoning evaluations are organized from deductive, inductive, abductive and mixed-form views. We select fifteen logical reasoning datasets to evaluate on three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) under both zero-shot and few-shot settings.
> - Second, this paper provides fine-level evaluations on four metrics, covering both objective and subjective views. For problematic cases, extensive error attributions are conducted from two dimensions, forming five error types. It uncovers the logical flaws of LLMs and we provide deep analysis on the results.
> - Third, to achieve a fair and pure benchmark for logical reasoning capability, we propose a dataset with neutral content, covering deductive, inductive and abductive settings.




## ğŸš€ NeuLR


Considering the current benchmarks may not provide neutral content for fair evaluation, we propose the new dataset ***NeuLR*** to benchmark the neutral-content logical reasoning tasks. In column 1âˆ¼3 of the able, we provide the statistics of NeuLR. It contains 3 k samples in total, with ***1 k for deductive reasoning***, ***1 k for inductive reasoning*** and ***1k for abductive reasoning***.








<!--
## Citation

If you find it helpful, please kindly cite the paper.

```
@article{DBLP:journals/corr/abs-2306-09841,
  author       = {Fangzhi Xu and
                  Qika Lin and
                  Jiawei Han and
                  Tianzhe Zhao and
                  Jun Liu and
                  Erik Cambria},
  title        = {Are Large Language Models Really Good Logical Reasoners? {A} Comprehensive Evaluation and Beyond},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  volume       = {37},
  number       = {4},
  pages        = {1620--1634},
  year         = {2025}
}
```
--> 

